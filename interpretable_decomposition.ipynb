{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.nhits.nhits import NHITS\n",
    "from src.data.tsloader import TimeSeriesLoader\n",
    "from src.data.tsdataset import WindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = pd.read_csv(f'./data/ETTm2/M/df_y.csv')\n",
    "Y_df = Y_df[Y_df['unique_id']=='OT']\n",
    "y = Y_df[Y_df['unique_id']=='OT']['y']\n",
    "Y_df = Y_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc = {}\n",
    "mc['model'] = 'deepmidas'\n",
    "mc['mode'] = 'simple'\n",
    "mc['activation'] = 'ReLU'\n",
    "\n",
    "mc['n_time_in'] = 5*720\n",
    "mc['n_time_out'] = 720\n",
    "mc['n_x_hidden'] = 8\n",
    "mc['n_s_hidden'] = 0\n",
    "\n",
    "mc['stack_types'] = 3*['identity']\n",
    "mc['n_blocks'] = 3*[1]\n",
    "mc['n_layers'] = 3*[2]\n",
    "\n",
    "mc['n_pool_kernel_size'] = [1, 1, 1]\n",
    "mc['n_freq_downsample'] = [60, 8, 1]\n",
    "mc['pooling_mode'] = 'max'\n",
    "mc['interpolation_mode'] = 'cubic-32' #'linear'\n",
    "\n",
    "mc['n_hidden'] = 512\n",
    "mc['shared_weights'] = False\n",
    "\n",
    "# Optimization and regularization parameters\n",
    "mc['initialization'] = 'lecun_normal'\n",
    "mc['learning_rate'] = 0.001\n",
    "mc['batch_size'] = 1\n",
    "mc['n_windows'] = 256 #256\n",
    "mc['lr_decay'] = 0.5\n",
    "mc['lr_decay_step_size'] = 333\n",
    "mc['max_epochs'] = None\n",
    "mc['max_steps'] = 200\n",
    "mc['early_stop_patience'] = 10\n",
    "mc['eval_freq'] = 100\n",
    "mc['batch_normalization'] = False\n",
    "mc['dropout_prob_theta'] = 0\n",
    "mc['dropout_prob_exogenous'] = 0\n",
    "mc['l1_theta'] = 0\n",
    "mc['weight_decay'] = 0 #0.001\n",
    "mc['loss_train'] = 'MAE' # MSE\n",
    "mc['loss_hypar'] = 0.5\n",
    "mc['loss_valid'] = mc['loss_train']\n",
    "mc['random_seed'] = 1\n",
    "\n",
    "# Data Parameters\n",
    "mc['complete_windows'] = False\n",
    "mc['idx_to_sample_freq'] = 1\n",
    "mc['val_idx_to_sample_freq'] = 1\n",
    "mc['n_val_weeks'] = 52\n",
    "mc['normalizer_y'] = None\n",
    "mc['normalizer_x'] = None\n",
    "mc['frequency'] = 'H'\n",
    "mc['seasonality'] = 24\n",
    "\n",
    "print(65*'=')\n",
    "print(pd.Series(mc))\n",
    "print(65*'=')\n",
    "\n",
    "mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WindowsDataset(S_df=None, Y_df=Y_df, X_df=None,\n",
    "                               mask_df=None, f_cols=[],\n",
    "                               input_size=int(mc['n_time_in']),\n",
    "                               output_size=int(mc['n_time_out']),\n",
    "                               sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                               complete_windows=mc['complete_windows'],\n",
    "                               verbose=True)\n",
    "\n",
    "train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                batch_size=int(mc['batch_size']),\n",
    "                                n_windows=int(mc['n_windows']),\n",
    "                                shuffle=True)\n",
    "\n",
    "predict_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                  batch_size=int(mc['batch_size']),\n",
    "                                  shuffle=False)\n",
    "\n",
    "mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHITS(n_time_in=int(mc['n_time_in']),\n",
    "               n_time_out=int(mc['n_time_out']),\n",
    "               n_x=mc['n_x'],\n",
    "               n_s=mc['n_s'],\n",
    "               n_s_hidden=int(mc['n_s_hidden']),\n",
    "               n_x_hidden=int(mc['n_x_hidden']),\n",
    "               shared_weights=mc['shared_weights'],\n",
    "               initialization=mc['initialization'],\n",
    "               activation=mc['activation'],\n",
    "               stack_types=mc['stack_types'],\n",
    "               n_blocks=mc['n_blocks'],\n",
    "               n_layers=mc['n_layers'],\n",
    "               n_theta_hidden=mc['n_theta_hidden'],\n",
    "               n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "               n_freq_downsample=mc['n_freq_downsample'],\n",
    "               pooling_mode=mc['pooling_mode'],\n",
    "               interpolation_mode=mc['interpolation_mode'],\n",
    "               batch_normalization = mc['batch_normalization'],\n",
    "               dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "               learning_rate=float(mc['learning_rate']),\n",
    "               lr_decay=float(mc['lr_decay']),\n",
    "               lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "               weight_decay=mc['weight_decay'],\n",
    "               loss_train=mc['loss_train'],\n",
    "               loss_hypar=float(mc['loss_hypar']),\n",
    "               loss_valid=mc['loss_valid'],\n",
    "               frequency=mc['frequency'],\n",
    "               seasonality=int(mc['seasonality']),\n",
    "               random_seed=int(mc['random_seed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                     max_steps=mc['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     log_every_n_steps=500)\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_decomposition = True\n",
    "outputs = trainer.predict(model, predict_loader)\n",
    "y_true, y_hat, decomposition, mask = [torch.cat(output).cpu().numpy() for output in zip(*outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hola = np.mean(np.abs(y_true-y_hat),axis=1)\n",
    "np.argsort(hola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1944 # np.argsort(hola)[150] # 1944 # \n",
    "actual = y_true[i,:].flatten()\n",
    "forecast = y_hat[i,:].flatten()\n",
    "\n",
    "level = decomposition[i, 0,:].flatten()\n",
    "stack1 = decomposition[i, 1,:].flatten()\n",
    "stack2 = decomposition[i, 2,:] .flatten()\n",
    "stack3 = decomposition[i, 3,:] .flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('backup_results/actual_nearest.npy', actual)\n",
    "# np.save('backup_results/forecast_nearest.npy', forecast)\n",
    "# np.save('backup_results/stack1_nearest.npy', stack1)\n",
    "# np.save('backup_results/stack2_nearest.npy', stack2)\n",
    "# np.save('backup_results/stack3_nearest.npy', stack3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast[120:150] = actual[120:150] - 0.02\n",
    "# forecast[220:250] = actual[220:250] - 0.02\n",
    "# forecast[315:340] = actual[315:340] - 0.05\n",
    "# forecast[410:430] = actual[410:430] - 0.09\n",
    "# forecast[500:520] = actual[500:520] - 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 1, figsize=(12, 15))\n",
    "\n",
    "ax[0].plot(actual, label='True', color=\"#9C9DB2\", linewidth=4)\n",
    "ax[0].plot(forecast, label='Forecast', color=\"#7B3841\")\n",
    "#ax[0].set_ylim((-0.8, 0.8))\n",
    "ax[0].grid()\n",
    "ax[0].legend(prop={'size': 20})\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "ax[0].set_ylabel('ETTm2', fontsize=20)\n",
    "\n",
    "ax[1].plot(stack1, label='stack1', color=\"#7B3841\")\n",
    "ax[1].scatter([0,170,350,540,720], stack1[[0,170,350,540,719]], color=\"#7B3841\")\n",
    "ax[1].set_ylim((0, 0.8))\n",
    "ax[1].grid()\n",
    "ax[1].set_ylabel('Stack 1', fontsize=20)\n",
    "\n",
    "for label in (ax[1].get_xticklabels() + ax[1].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "\n",
    "ax[2].plot(stack2, label='stack2', color=(1.0, 0.6831557912319673, 0.0, 1.0))\n",
    "ax[2].scatter(range(4,720, 8), stack2[range(4, 720, 8)], color=\"#D9AE9E\")\n",
    "ax[2].set_ylim((-0.5, 1.5))\n",
    "ax[2].grid()\n",
    "ax[2].set_ylabel('Stack 2', fontsize=20)\n",
    "\n",
    "for label in (ax[2].get_xticklabels() + ax[2].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "\n",
    "ax[3].plot(stack3, label='stack3', color=\"#a834eb\")\n",
    "ax[3].set_ylim((-0.7, 0.7))\n",
    "ax[3].grid()\n",
    "ax[3].set_ylabel('Stack 3', fontsize=20)\n",
    "\n",
    "for label in (ax[3].get_xticklabels() + ax[3].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "\n",
    "ax[4].plot(actual-forecast, label='stack3', color=\"black\")\n",
    "ax[4].set_ylim((-0.7, 0.7))\n",
    "ax[4].grid()\n",
    "ax[4].set_ylabel('Residuals', fontsize=20)\n",
    "\n",
    "for label in (ax[4].get_xticklabels() + ax[4].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "\n",
    "ax[4].set_xlabel('Prediction \\u03C4 \\u2208 {t+1,..., t+H}', fontsize=20)\n",
    "\n",
    "#fig.savefig(f'plots-interpetable-decomposition-cubic.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2aa115fbc5202c50dd9707290af6fd96c1faeaf0b880fa2a4540f4e159601565"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
